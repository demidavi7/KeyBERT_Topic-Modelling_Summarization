# -*- coding: utf-8 -*-
"""summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C2lIxKO50QyUOSa40bEGT0EXBMIfwVvu
"""

import torch
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from transformers import BartTokenizer, BartForConditionalGeneration

def summarize_with_keywords(
    text: str,
    model_type: str = "single",
    top_n_keywords: int = 10,
    keyphrase_ngram_range: tuple = (1, 2),
    stop_words: str = "english",
    use_mmr: bool = True,
    diversity: float = 0.5,
    max_length: int = 128,
    num_beams: int = 4
) -> str:
    """
    Given an input text, automatically extracts keywords with KeyBERT,
    inserts them into the text according to the specified model ('single'
    or 'list'), and generates a summary using the corresponding fine-tuned
    BART models:
        - VexPoli/distilbart-summarization-top-single
        - VexPoli/distilbart-summarization-top-list

    Parameters
    ----------
    text : str
        The input text to summarize.
    model_type : str
        Either 'single' (one <keyword>...</keyword> block) or 'list'
        (multiple <keyword>...</keyword> blocks).
    top_n_keywords : int
        How many keywords to extract from the text.
    keyphrase_ngram_range : tuple
        The n-gram range for KeyBERT (e.g., (1,2)).
    stop_words : str
        Which stop words language to remove in KeyBERT.
    use_mmr : bool
        Whether to use Maximal Marginal Relevance for reducing redundancy.
    diversity : float
        Diversity for MMR (0.0 = minimal diversity, 1.0 = maximal).
    max_length : int
        Maximum length of the generated summary.
    num_beams : int
        Number of beams for beam search generation.

    Returns
    -------
    summary : str
        The generated summary.

    Example
    -------
    summary = summarize_with_keywords(
        text=\"Manchester City ...\",
        model_type=\"single\"
    )
    print(summary)
    """
    # 1. Choose model based on 'model_type'
    if model_type.lower() == "single":
        model_name = "VexPoli/distilbart-summarization-top-single"
    elif model_type.lower() == "list":
        model_name = "VexPoli/distilbart-summarization-top-list"
    else:
        raise ValueError("model_type must be either 'single' or 'list'.")

    # Load Summarization Model & Tokenizer
    tokenizer = BartTokenizer.from_pretrained(model_name)
    model = BartForConditionalGeneration.from_pretrained(model_name)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()

    # 2. Initialize KeyBERT
    sbert_model = SentenceTransformer("all-MiniLM-L6-v2")
    kw_model = KeyBERT(model=sbert_model)

    # 3. Extract keywords
    keywords = kw_model.extract_keywords(
        text,
        keyphrase_ngram_range=keyphrase_ngram_range,
        stop_words=stop_words,
        top_n=top_n_keywords,
        use_mmr=use_mmr,
        diversity=diversity
    )

    # 4. Enrich text based on model_type
    if model_type.lower() == "single":
        # Single <keyword> block
        kw_list = [kw[0].upper() for kw in keywords]
        kw_block = "<keyword>" + ",".join(kw_list) + "</keyword>"
        enriched_text = f"{kw_block}\n\n{text}"
    else:
        # 'list': each keyword in its own tag
        kw_list = [kw[0].upper() for kw in keywords]
        list_tokens = " ".join([f"<keyword>{k}</keyword>" for k in kw_list])
        enriched_text = f"{list_tokens}\n\n{text}"

    # 5. Tokenize enriched text
    inputs = tokenizer(
        enriched_text,
        return_tensors="pt",
        max_length=512,
        truncation=True
    )
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # 6. Generate the summary
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_length,
            num_beams=num_beams,
            early_stopping=True
        )

    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return summary