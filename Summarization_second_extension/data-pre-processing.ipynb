{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keybert","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T00:45:33.877518Z","iopub.execute_input":"2025-01-09T00:45:33.877903Z","iopub.status.idle":"2025-01-09T00:45:37.121975Z","shell.execute_reply.started":"2025-01-09T00:45:33.877873Z","shell.execute_reply":"2025-01-09T00:45:37.121112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset building \n\n# the dataset is a sub-sample of newspaper-text-summarization-cnn-dailymail train.csv \n# contains 200 MB of records with a size of 50000 samples","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\"\nds = pd.read_csv(file_path)\nsample_size = 50000\nsampled_ds = ds.sample(n=sample_size, random_state=42)\nprint(sampled_ds.head())\noutput_path = \"summarization_dataset.csv\"\nsampled_ds.to_csv(output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T00:45:37.123343Z","iopub.execute_input":"2025-01-09T00:45:37.123589Z","iopub.status.idle":"2025-01-09T00:45:55.735412Z","shell.execute_reply.started":"2025-01-09T00:45:37.123565Z","shell.execute_reply":"2025-01-09T00:45:55.734509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## enriching the dataset with keywords at the end of the text","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport gc\nfrom typing import List, Dict\nfrom keybert import KeyBERT\nfrom sentence_transformers import SentenceTransformer\nprint(\"Checking and importing dependencies...\")\nprint(f\"PyTorch version: {torch.__version__}\")\n\ndef process_texts(articles: list, summaries: list, batch_size: int, top_n_keywords: int, model_path: str):\n    \"\"\"\n    Process articles with formatted keywords appended to the text for special tokenization.\n    \"\"\"\n    # Enable performance optimizations\n    torch.backends.cudnn.benchmark = True\n    if hasattr(torch.backends.cuda, 'matmul'):\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Initialize model with optimizations\n    try:\n        model = SentenceTransformer(model_path)\n        if device.type == 'cuda':\n            model.half()  # Use FP16 for faster inference\n        model.to(device)\n        kw_model = KeyBERT(model=model)\n    except Exception as e:\n        print(f\"Error initializing models: {e}\")\n        raise\n\n    results = []\n\n    # Process in batches\n    for i in tqdm(range(0, len(articles), batch_size), desc=\"Processing batches\"):\n        batch_articles = articles[i:i + batch_size]\n        batch_summaries = summaries[i:i + batch_size]\n\n        try:\n            # Process each article in the batch\n            for idx, (article, summary) in enumerate(zip(batch_articles, batch_summaries)):\n                try:\n                    keywords = kw_model.extract_keywords(\n                        article,\n                        keyphrase_ngram_range=(1, 2),\n                        stop_words='english',\n                        top_n=top_n_keywords,\n                        use_maxsum=False,\n                        use_mmr=True,\n                        diversity=0.5\n                    )\n\n                    # Format keywords as special tokens\n                    formatted_keywords = \", \".join(\n                        [f\"<keyword>{kw.upper()}</keyword>\" for kw, score in keywords]\n                    )\n\n                    # Append the keywords to the article text\n                    augmented_text = f\"{article}\\n\\nKeywords: {formatted_keywords}\"\n                    \n                    # Append result (no 'id' field included)\n                    results.append({\"text\": augmented_text, \"highlights\": summary})\n\n                except Exception as e:\n                    print(f\"Error processing article {i+idx}: {e}\")\n                    results.append({\"text\": article, \"highlights\": summary})\n\n            if i % (batch_size * 4) == 0 and i > 0 and device.type == 'cuda':\n                torch.cuda.empty_cache()\n                gc.collect()\n\n        except Exception as e:\n            print(f\"Error processing batch starting at index {i}: {e}\")\n            for article, summary in zip(batch_articles, batch_summaries):\n                results.append({\"text\": article, \"highlights\": summary})\n\n    return results\n\n\n\ndef process_dataset(file_path: str, batch_size: int, top_n_keywords: int, model_path: str) -> List[Dict]:\n\n    print(\"Loading dataset...\")\n    df = pd.read_csv(file_path)\n    \n    articles = df[\"article\"].tolist()\n    summaries = df[\"highlights\"].tolist()\n\n    return process_texts(articles, summaries, batch_size, top_n_keywords, model_path)\n\n\n\ndef save_to_csv(processed_data: List[Dict], output_path: str):\n    chunk_size = 5000\n    for i in range(0, len(processed_data), chunk_size):\n        chunk = processed_data[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n\n        pd.DataFrame(chunk).to_csv(\n            output_path,\n            index=False,\n            mode=mode,\n            header=header\n        )\n\n        del chunk\n        gc.collect()\n\n    print(f\"Saved processed data to {output_path}\")\n\n\ndef main():\n    print(\"\\nCUDA Information:\")\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n\n    file_path = \"summarization_dataset.csv\"\n\n    processed_data = process_dataset(\n        file_path=file_path,\n        batch_size=64, \n        top_n_keywords=10,\n        model_path=\"all-MiniLM-L6-v2\"\n    )\n\n    # Save results\n    output_path = \"processed_dataset_bottom.csv\"\n    save_to_csv(processed_data, output_path)\n\n    print(f\"\\nProcessing completed. Processed {len(processed_data)} documents\")\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T00:45:55.748223Z","iopub.execute_input":"2025-01-09T00:45:55.748455Z","iopub.status.idle":"2025-01-09T03:42:27.779683Z","shell.execute_reply.started":"2025-01-09T00:45:55.748432Z","shell.execute_reply":"2025-01-09T03:42:27.778791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## enriching the dataset with keywords at the beginning of the text ","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport gc\nfrom typing import List, Dict\nfrom keybert import KeyBERT\nfrom sentence_transformers import SentenceTransformer\n\ndef process_texts(articles: list, summaries: list, batch_size: int, top_n_keywords: int, model_path: str):\n    \"\"\"\n    Process articles with formatted keywords appended to the text for special tokenization.\n    \"\"\"\n    # Enable performance optimizations\n    torch.backends.cudnn.benchmark = True\n    if hasattr(torch.backends.cuda, 'matmul'):\n        torch.backends.cuda.matmul.allow_tf32 = True\n\n    # Setup GPU\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Initialize model with optimizations\n    try:\n        model = SentenceTransformer(model_path)\n        if device.type == 'cuda':\n            model.half()  # Use FP16 for faster inference\n        model.to(device)\n        kw_model = KeyBERT(model=model)\n    except Exception as e:\n        print(f\"Error initializing models: {e}\")\n        raise\n\n    results = []\n\n    # Process in batches\n    for i in tqdm(range(0, len(articles), batch_size), desc=\"Processing batches\"):\n        batch_articles = articles[i:i + batch_size]\n        batch_summaries = summaries[i:i + batch_size]\n\n        try:\n            # Process each article in the batch\n            for idx, (article, summary) in enumerate(zip(batch_articles, batch_summaries)):\n                try:\n                    keywords = kw_model.extract_keywords(\n                        article,\n                        keyphrase_ngram_range=(1, 2),\n                        stop_words='english',\n                        top_n=top_n_keywords,\n                        use_maxsum=False,\n                        use_mmr=True,\n                        diversity=0.5\n                    )\n\n                    # Format keywords as special tokens\n                    formatted_keywords = \", \".join(\n                        [f\"<keyword>{kw.upper()}</keyword>\" for kw, score in keywords]\n                    )\n\n                    # Append the keywords to the article text\n                    augmented_text = f\"Keywords: {formatted_keywords}\\n\\n{article}\"\n                    \n                    # Append result (no 'id' field included)\n                    results.append({\"text\": augmented_text, \"highlights\": summary})\n\n                except Exception as e:\n                    print(f\"Error processing article {i+idx}: {e}\")\n                    results.append({\"text\": article, \"highlights\": summary})\n\n            if i % (batch_size * 4) == 0 and i > 0 and device.type == 'cuda':\n                torch.cuda.empty_cache()\n                gc.collect()\n\n        except Exception as e:\n            print(f\"Error processing batch starting at index {i}: {e}\")\n            for article, summary in zip(batch_articles, batch_summaries):\n                results.append({\"text\": article, \"highlights\": summary})\n\n    return results\n\n\n\ndef process_dataset(file_path: str, batch_size: int, top_n_keywords: int, model_path: str) -> List[Dict]:\n    \n    print(\"Loading dataset...\")\n    df = pd.read_csv(file_path)\n    # Assicurati che le colonne siano denominate correttamente\n    articles = df[\"article\"].tolist()\n    summaries = df[\"highlights\"].tolist()\n\n    return process_texts(articles, summaries, batch_size, top_n_keywords, model_path)\n\n\n\ndef save_to_csv(processed_data: List[Dict], output_path: str):\n    chunk_size = 5000\n    for i in range(0, len(processed_data), chunk_size):\n        chunk = processed_data[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n\n        pd.DataFrame(chunk).to_csv(\n            output_path,\n            index=False,\n            mode=mode,\n            header=header\n        )\n\n        del chunk\n        gc.collect()\n\n    print(f\"Saved processed data to {output_path}\")\n\n\ndef main():\n    # Print CUDA information\n    print(\"\\nCUDA Information:\")\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n\n    file_path = \"summarization_dataset.csv\"\n\n    # Process dataset with optimized parameters for single GPU\n    processed_data = process_dataset(\n        file_path=file_path,\n        batch_size=64,  \n        top_n_keywords=10,\n        model_path=\"all-MiniLM-L6-v2\"\n    )\n\n    # Save results\n    output_path = \"processed_dataset_top.csv\"\n    save_to_csv(processed_data, output_path)\n\n    print(f\"\\nProcessing completed. Processed {len(processed_data)} documents\")\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T03:42:27.782283Z","iopub.execute_input":"2025-01-09T03:42:27.782525Z","iopub.status.idle":"2025-01-09T06:38:32.341024Z","shell.execute_reply.started":"2025-01-09T03:42:27.782504Z","shell.execute_reply":"2025-01-09T06:38:32.340143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making a dataset with list of keywords as unique special token\n","metadata":{}},{"cell_type":"code","source":"import re\nfrom datasets import load_dataset\nfrom huggingface_hub import login\n\ndef combine_keywords(example):\n    text = example[\"text\"]\n    keyword_matches = re.findall(r'<keyword>(.*?)</keyword>', text)\n\n    if keyword_matches:\n        seen = set()\n        ordered_keywords = []\n        for match in keyword_matches:\n            for keyword in match.split(','):\n                keyword = keyword.strip()\n                if keyword and keyword not in seen:\n                    seen.add(keyword)\n                    ordered_keywords.append(keyword)\n                    \n        combined = ','.join(ordered_keywords)\n        new_block = f\"Keywords: <keyword>{combined}</keyword>\"\n\n        modified_text = re.sub(\n            r'^Keywords:\\s*(?:<keyword>.*?</keyword>\\s*,?\\s*)+',\n            new_block,\n            text,\n            flags=re.DOTALL\n        )\n        example[\"text\"] = modified_text\n\n    return example\n\n\nds = load_dataset(\"VexPoli/cnn_enrich_with_top_keywords\")\nmodified_ds = ds.map(combine_keywords)\nmodified_ds\nlogin(token=\"\")\nmodified_ds.push_to_hub(\"cnn_enrich_with_top_keywords_modified\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}