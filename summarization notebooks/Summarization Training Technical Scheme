technical scheme summarization:

Parameters: 305,513,472
Trainable parameters : 51,473,408

The input texts are truncated to a maximum of 1024 tokens, while the highlights (summaries) are truncated to 256 tokens. The labels are then attached to the tokenized input dictionary.
Our code uses a distilled version of BART, "sshleifer/distilbart-cnn-12-6", which is suitable for summarization tasks.

Tokenizer Modifications:
The tokenizer is extended with additional special tokens (<keyword> and </keyword>) to mark or emphasize keywords in the data.

Model Adjustments:
The model’s token embeddings are resized to account for the newly added special tokens.
The decoder start token, end-of-sequence (EOS), and padding tokens are explicitly set to ensure proper decoding behavior.

Parameter Freezing:
All parameters of the model are frozen (i.e., their gradients are disabled) so that the base layers do not update during fine-tuning.
Only the shared embedding layer and the final linear layer (lm_head) are unfrozen, meaning that only these parts will be updated during training.


Loss Computation:
The model’s output logits are compared with the labels using a Cross-Entropy loss function.

Keyword Weighting:
A mask is created that identifies tokens corresponding to the additional special tokens (keywords). The loss for these tokens is weighted by a factor (multiplied by 2.5) compared to the loss for other tokens. This encourages the model to pay extra attention to the keywords during training.

LEARNING PARAMS : 
num_train_epochs: The model is trained for 2 epochs.
optim: Uses the “adafactor” optimizer, which is well-suited for transformer models and can be more memory efficient.
learning_rate: Set to a very low value (1e-5) to ensure gentle fine-tuning.
